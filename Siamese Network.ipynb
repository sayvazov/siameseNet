{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:60: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\legacy\\layers.py:460: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:63: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[/input_11..., outputs=sigmoid.0)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2921985\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Conv2D, Lambda,merge, Dense, Flatten, MaxPooling2D \n",
    "from keras.layers.merge import Add\n",
    "from keras.regularizers import l2\n",
    "from keras import backend\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.losses import binary_crossentropy\n",
    "\n",
    "import numpy as np\n",
    "import numpy.random \n",
    "import os\n",
    "import dill as pickle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "\n",
    "print(\"Starting\")\n",
    "\n",
    "ata = fetch_olivetti_faces()\n",
    "\n",
    "def initialWeights(shape, name=None):\n",
    "    #Initialization as in paper\n",
    "    values = numpy.random.normal(loc=0, scale=0.01, size=shape)\n",
    "    return backend.variable(values, name=name)\n",
    "\n",
    "def initialBiases(shape, name=None):\n",
    "    #Initialization as in paper\n",
    "    values = numpy.random.normal(loc=0.5, scale=0.01, size=shape)\n",
    "    return backend.variable(values, name)\n",
    "\n",
    "\n",
    "imageSize = 64\n",
    "def buildNet():\n",
    "    #Olivetti faces are 64 x 64 pixels\n",
    "    inputShape = (imageSize, imageSize, 1)\n",
    "    leftInput  = Input(inputShape)\n",
    "    rightInput = Input(inputShape)\n",
    "    convolutionNet = Sequential()\n",
    "    #Testing was done with much smaller layers, for speed. \n",
    "    #The same network is used for both sides of the siamese network, here initialized with convolution layers\n",
    "    convolutionNet.add(Conv2D(64, (9, 9), activation='relu', input_shape = inputShape, kernel_initializer = initialWeights, bias_initializer = initialBiases ) )\n",
    "    convolutionNet.add(MaxPooling2D(2))\n",
    "    convolutionNet.add(Conv2D(128, (7, 7), activation='relu', input_shape = inputShape, kernel_initializer = initialWeights, bias_initializer = initialBiases ) )\n",
    "    convolutionNet.add(MaxPooling2D(2))\n",
    "    convolutionNet.add(Conv2D(128, (4, 4), activation='relu', input_shape = inputShape, kernel_initializer = initialWeights, bias_initializer = initialBiases ) )\n",
    "    convolutionNet.add(MaxPooling2D(2))\n",
    "    convolutionNet.add(Conv2D(128, (3, 3), activation='relu', input_shape = inputShape, kernel_initializer = initialWeights, bias_initializer = initialBiases ) )\n",
    "    convolutionNet.add(Flatten())\n",
    "    #Dense combination before comparison\n",
    "    convolutionNet.add(Dense(4096, activation=\"sigmoid\", kernel_regularizer=l2(0.001), kernel_initializer= initialWeights, bias_initializer= initialBiases))\n",
    "    \n",
    "    #Convolutional net is applied to both inputs\n",
    "    leftResult = convolutionNet(leftInput)\n",
    "    rightResult= convolutionNet(rightInput)\n",
    "    \n",
    "    #Then the difference is taken. Merge is a deprecated layer, but I could not figure out the non-deprecated way of doing this\n",
    "    L1 = lambda x: backend.abs(x[0] - x[1])\n",
    "    first = lambda x: x[0]\n",
    "    distance = merge([leftResult, rightResult], mode=L1, output_shape = first )\n",
    "    #The distance is then used to predict the probability of the images being of the same class\n",
    "    prediction = Dense(1, activation=\"sigmoid\", bias_initializer = initialBiases, kernel_initializer= initialWeights) (distance)\n",
    "    siameseNet = Model(input=[leftInput, rightInput], output = prediction)\n",
    "    \n",
    "    optimizer = SGD()\n",
    "    siameseNet.compile(loss=\"binary_crossentropy\", optimizer = optimizer)\n",
    "    siameseNet.count_params()\n",
    "    return siameseNet\n",
    "\n",
    "siameseNet = buildNet()\n",
    "#Full network has 3,000,000 free parameters. The test network had about 14,000\n",
    "print(siameseNet.count_params())\n",
    "\n",
    "#Curently no hyper-parameter optimization, learning decay, or affine distortions\n",
    "\n",
    "\n",
    "#There are 400 images, with 10 images in each class. We will take 24 classes (240 images, 60% of data)\n",
    "#to use to build our network. 60% is taken from the paper.\n",
    "numberOfClasses = 40\n",
    "numberTrainingClasses = 24\n",
    "numberTestClasses = numberOfClasses - numberTrainingClasses\n",
    "classSize = 10\n",
    "trainingData = data.images[:numberTrainingClasses*classSize]\n",
    "trainingLabels = data.target[:numberTrainingClasses*classSize]\n",
    "testData = data.images[numberTrainingClasses*classSize:]\n",
    "testLabels =data.target[numberTrainingClasses*classSize:]\n",
    "\n",
    "#Generates a batch of tests, half from the same class, half from different classes. \n",
    "def generateBatch(size):\n",
    "    pairs = [np.zeros((size, imageSize, imageSize, 1)), np.zeros((size, imageSize,imageSize, 1))]\n",
    "    targets= np.zeros(size)\n",
    "    for i in range(size):\n",
    "        category1 = numpy.random.randint(numberTrainingClasses)\n",
    "        item1 = numpy.random.randint ( classSize)\n",
    "        #if even element, choose another element from same category. If odd, pick from other category\n",
    "        if ( i % 2 == 0):\n",
    "            category2 = category1\n",
    "            item2 = (item1 + numpy.random.randint (1, classSize)) % classSize\n",
    "            targets[i] = 1\n",
    "        else:\n",
    "            category2 = (category1 + numpy.random.randint(1, numberTrainingClasses)) % numberTrainingClasses\n",
    "            item2 = numpy.random.randint ( classSize)\n",
    "        pairs[0][i, :, :, :] = trainingData[category1*classSize + item1].reshape(imageSize, imageSize, 1)\n",
    "        pairs[1][i, :, :, :] = trainingData[category2*classSize + item2].reshape(imageSize, imageSize, 1)\n",
    "    return pairs, targets\n",
    "    \n",
    "#Generates one example from each test class to compare against\n",
    "def generateOneShotSamples():\n",
    "    classExamples = numpy.zeros((numberTestClasses, imageSize, imageSize, 1))\n",
    "    indices = numpy.zeros(numberTestClasses)\n",
    "    for i in range(numberTestClasses):\n",
    "        indices[i] = numpy.random.randint(classSize)\n",
    "        classExamples[i, :, :, :] = testData[i*classSize + indices[i]].reshape(imageSize, imageSize, 1)\n",
    "    return (indices, classExamples)\n",
    "\n",
    "#Generates examples from test set that are not the excluded indices, so that \n",
    "#we do not try to categorize an item as itself\n",
    "def generateOneShotTests(size, indicesExclude):\n",
    "    tests = numpy.zeros((size, imageSize, imageSize, 1))\n",
    "    labels = numpy.zeros(size)\n",
    "    for i in range(size):\n",
    "        index = numpy.random.randint(len(testData))\n",
    "        answer = testLabels[index] \n",
    "        while(index % classSize == indicesExclude[answer- numberTrainingClasses]):\n",
    "            index = numpy.random.randint(len(testData))\n",
    "            answer = testLabels[index]\n",
    "        tests[i,:,:,:] = testData[index].reshape(imageSize, imageSize, 1)\n",
    "        labels[i] = answer\n",
    "    return (tests, labels)\n",
    "    \n",
    "#How often to check on new classes and how large to make each check\n",
    "checkFrequency = 100\n",
    "checkSize = 2* numberTestClasses \n",
    "\n",
    "#Force into the form that Keras expects. No real math being done here\n",
    "def makePairs(test, examples):\n",
    "    \n",
    "    currentTestPairs = []\n",
    "    for j in range (len(examples)):\n",
    "        currentTestPairs.append([numpy.array([tests[j]]), numpy.array([examples[j]])])\n",
    "    return (currentTestPairs)\n",
    "\n",
    "\n",
    "#Actually run the code, for a thousand iterations\n",
    "loss = numpy.zeros(1000)\n",
    "testError = []\n",
    "#First for loop for output test\n",
    "#for i in range(1):\n",
    "for i in range(1000):\n",
    "    (inputs, targets) = generateBatch(24)\n",
    "    loss[i] = siameseNet.train_on_batch(inputs, targets)\n",
    "    print(\"iteration \" + str(i) + \" with loss \" + str(loss[i]))\n",
    "    #If it is time to check on the test set, do so. \n",
    "    if ( i % checkFrequency == 0):\n",
    "        indices, examples = generateOneShotSamples()\n",
    "        tests, labels = generateOneShotTests(checkSize, indices)\n",
    "        numCorrect = 0\n",
    "        for j in range(checkSize):\n",
    "            #This is obviously a bit of a hack, but I could not figure out how to \n",
    "            # bundle the batches together properly, so I am running predict on each example \n",
    "            # individualy\n",
    "            #print(\"Testing on iteration \" + str(i) + \" and trial \" + str(j))\n",
    "            testPairs = makePairs(tests[j], examples)\n",
    "            probabilities = numpy.zeros(numberTestClasses)\n",
    "            for k in range (numberTestClasses):\n",
    "                probabilities[k] = siameseNet.predict(testPairs[k], numberTestClasses)\n",
    "            if (np.argmax(probabilities) == labels[j]):\n",
    "                numCorrect+=1\n",
    "        accuracy = numCorrect*100 / checkSize\n",
    "        testError.append(accuracy)\n",
    "        print(\"Testing on iteration \" + str(i) + \" with accuracy \" + str(accuracy))\n",
    "plt.figure()\n",
    "plt.plot(loss)\n",
    "plt.show()\n",
    "plt.plot(testError)\n",
    "plt.show()\n",
    "# To be entirely honest, I have not been able to run this on a large enough machine, \n",
    "# so I have not seen the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
